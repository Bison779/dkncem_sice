{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Deep Koopman model using the Keras framework\n",
    "\n",
    "\n",
    "## Method based on the following paper (aka, ULE)\n",
    "Lusch, B.; Kutz, J. N. & Brunton, S. L.\n",
    "Deep learning for universal linear embeddings of nonlinear dynamics \n",
    "Nature communications, Nature Publishing Group, 2018, 9, 1-10\n",
    "\n",
    "## Original code by dykua \n",
    "https://github.com/dykuang/Deep----Koopman\n",
    "\n",
    "## Modifications by Brendan and Katharina 26/8/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All background processes were killed.\n"
     ]
    }
   ],
   "source": [
    "%killbgscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 23:45:57.409334: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 23:45:58.033592: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-24 23:45:58.033635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-24 23:45:58.033705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:45:58.034106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 80 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 849.46GiB/s\n",
      "2022-03-24 23:45:58.034119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-24 23:45:58.035257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-24 23:45:58.035282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-24 23:45:58.035777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-24 23:45:58.035881: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-24 23:45:58.037087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-24 23:45:58.037351: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-24 23:45:58.037421: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-24 23:45:58.037464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:45:58.037825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:45:58.038154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Needed for running custom loss\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Fix the seed, for evaluation\n",
    "# Remove this to explore generalisability\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pickle\n",
    "import itertools\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generator libraries\n",
    "import sys\n",
    "import h5py\n",
    "sys.path.append('../fromServer_Keras-HDF5-ImageDataGenerator/')\n",
    "from h5imagegenerator.generator import HDF5ImageGenerator\n",
    "from albumentations import Compose,Crop,ToFloat,Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation, image\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "BATCH_SIZE = 128 # BEST for state, 2 seems to never converge\n",
    "\n",
    "EPOCHS = 25\n",
    "\n",
    "# Define the number of historical timesteps used for learning\n",
    "NUM_HISTORICAL_TRAINING_INPUT_STEPS = 1\n",
    "\n",
    "# Define the number of timesteps in the latent space (should be same as input and output)\n",
    "NUM_HISTORICAL_TRAINING_LATENT_STEPS = NUM_HISTORICAL_TRAINING_INPUT_STEPS\n",
    "# Define the number of timesteps in the output (should be same as input and latent)\n",
    "NUM_HISTORICAL_TRAINING_OUTPUT_STEPS = NUM_HISTORICAL_TRAINING_INPUT_STEPS \n",
    "\n",
    "# Define the number of timesteps used to compute the prediction loss (is 30 in ULE paper)\n",
    "NUM_HISTORICAL_LOSS_PREDICTION_STEPS = NUM_HISTORICAL_TRAINING_INPUT_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of states in the input\n",
    "xmax_state = 4 # This assumes the final state is the control\n",
    "num_control = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose dataset\n",
    "# dataset = \"20220308_beamwall\"\n",
    "dataset = \"20220309_easy\"\n",
    "directory = '../src/{}'.format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset parameters (don't need to modify)\n",
    "X_key = 'state_t'\n",
    "y_key = 'state_tp1'\n",
    "classes_key = 'state_tp1'\n",
    "\n",
    "# Augmentor to set the maximum size of the state, and the number of historical data used for training\n",
    "augmentor = Compose([\n",
    "    Crop(x_min=0, y_min=0, x_max=xmax_state, y_max=NUM_HISTORICAL_TRAINING_INPUT_STEPS),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generators on the HDF5 files (don't need to modify)\n",
    "train_generator = HDF5ImageGenerator(\n",
    "        src = directory+'/train.hdf5',\n",
    "        X_key=X_key,\n",
    "        y_key=y_key,\n",
    "        classes_key = classes_key,\n",
    "        scaler=False, # Don't normalise data. If you want to do this, use the augmentor above\n",
    "        labels_encoding=False, # this is fine, as its regression\n",
    "        batch_size=BATCH_SIZE,\n",
    "        mode='train',\n",
    "        augmenter=augmentor,\n",
    "        augmenter_labels = augmentor, # also crop the outputs, modificated by Brendan\n",
    "        shuffle=True,\n",
    ")\n",
    "\n",
    "valid_generator = HDF5ImageGenerator(\n",
    "        src=directory+'/valid.hdf5',\n",
    "        X_key=X_key,\n",
    "        y_key=y_key,\n",
    "        classes_key = classes_key,\n",
    "        scaler=False,\n",
    "        labels_encoding=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        mode='train',\n",
    "        augmenter=augmentor,\n",
    "        augmenter_labels = augmentor,\n",
    "        shuffle=True,\n",
    ")\n",
    "\n",
    "test_generator = HDF5ImageGenerator(\n",
    "        src=directory+'/test.hdf5',\n",
    "        X_key=X_key,\n",
    "        y_key=y_key,\n",
    "        classes_key = classes_key,\n",
    "        scaler=False,\n",
    "        labels_encoding=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        mode='train', \n",
    "        augmenter=augmentor,\n",
    "        augmenter_labels = augmentor, \n",
    "        shuffle=True,\n",
    ")\n",
    "\n",
    "EVALUATION_INTERVAL = len(train_generator._indices) // BATCH_SIZE\n",
    "VALIDATION_STEPS = len(valid_generator._indices) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3013636940312392 1.226839275730759\n",
      "-1.2613272475469868 1.266432958081564\n",
      "-1.3668266527313806 0.9714522048917758\n",
      "-1.4036331130047328 0.9832533627174338\n",
      "-0.2213204542743128 0.34039393067359924\n",
      "-0.22173959413637334 0.3391719162464142\n",
      "(128, 1, 4)\n",
      "(128, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "# Print some ranges to explore the data\n",
    "print(train_generator[0][0][:,:,0].min(),train_generator[0][0][:,:,0].max())\n",
    "print(train_generator[0][1][:,:,0].min(),train_generator[0][1][:,:,0].max())\n",
    "print(train_generator[0][0][:,:,1].min(),train_generator[0][0][:,:,1].max())\n",
    "print(train_generator[0][1][:,:,1].min(),train_generator[0][1][:,:,1].max())\n",
    "print(train_generator[0][0][:,:,2].min(),train_generator[0][0][:,:,2].max())\n",
    "print(train_generator[0][1][:,:,2].min(),train_generator[0][1][:,:,2].max())\n",
    "\n",
    "# View the shape of a batch\n",
    "print(train_generator[0][0].shape) # Note train_generator[i][0] is the input (xt) data for batch i\n",
    "print(train_generator[0][1].shape) # Note train_generator[i][1] is the output (xt+1) data for batch i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best settings I've found so far for the 2D state data\n",
    "par = {\n",
    "       'input steps':NUM_HISTORICAL_TRAINING_INPUT_STEPS,\n",
    "       'latent steps':NUM_HISTORICAL_TRAINING_LATENT_STEPS,\n",
    "       'output steps':NUM_HISTORICAL_TRAINING_OUTPUT_STEPS,\n",
    "       'pred steps': NUM_HISTORICAL_LOSS_PREDICTION_STEPS,\n",
    "       'batch size': BATCH_SIZE,\n",
    "       'en dim list': [80,80], # Standard size\n",
    "       'de dim list': [80,80], # Standard size\n",
    "#        'en dim list': [64,128,256],  # Large size\n",
    "#        'de dim list': [256,128,64], # Large size\n",
    "       'K reg': 10e-14, # Regularization parameter for auxiliary Koopman network \n",
    "       'epochs': EPOCHS,\n",
    "       'num complex': 10,  # hyperparameter for number of conjugate pairs (2 needed for rigid pendulum (no control), unclear how many needed for soft)\n",
    "       'num real': 0, # hyperparameter for real block (not implemented yet, but also might not be needed)\n",
    "       'hidden_widths_omega': 170, # the width of the auxiliary network (only one layer)\n",
    "       'lr': 0.001,\n",
    "       'alpha_1' : 0.001, # Trade off between prediction and linear error (see loss)\n",
    "       #'alpha_2' : 10e-9, # this is an infinity norm term from ULE paper, but its not used in the code\n",
    "       'alpha_3' : 10e-14,\n",
    "       }\n",
    "par['num_samples'], par['time steps'], par['input feature dim'] = train_generator[0][0].shape\n",
    "_, _, par['output feature dim'] = train_generator[0][1].shape\n",
    "input_shape = (par['input steps'], par['input feature dim'])\n",
    "output_shape = (par['output steps'], par['output feature dim'])\n",
    "\n",
    "# Latent dimension\n",
    "par['latent dim'] = 2*par['num complex'] + par['num real']\n",
    "latent_shape = (par['latent steps'],)+ (par['latent dim'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Reshape\n",
    "from Architecture_old import _transformer,_pred_K, linear_update\n",
    "'''\n",
    "Losses\n",
    "'''\n",
    "from keras.losses import mean_squared_error, mean_absolute_error\n",
    "def S_error(args):\n",
    "    Y0, Y1 = args\n",
    "    return tf.reduce_mean(tf.math.squared_difference(Y0,Y1))\n",
    "def State_loss(yTrue, yPred):\n",
    "    return tf.reduce_mean(tf.math.squared_difference(encoder(yTrue),KGx))\n",
    "# The plain old reconstruction loss\n",
    "def Rec_loss(yTrue, yPred):\n",
    "    return tf.reduce_mean(tf.math.squared_difference(x_in[:,:,:xmax_state-num_control],decoded_x[:,:,:xmax_state-num_control]))\n",
    "# The mean reconstruction loss over m steps into the future\n",
    "def Rec_plus1_loss(yTrue, yPred):\n",
    "    return tf.reduce_mean(tf.math.squared_difference(yTrue[:,:,:xmax_state-num_control],yPred[:,:,:xmax_state-num_control]))\n",
    "def customLoss():\n",
    "    # yTrue is the actual value X_t+1\n",
    "    # yPred is the predicted reconstruction at X_t+1 (as this is the output of full_model, aka decoded_xp)\n",
    "    # However, yPred is not actally needed when computing the (custom) loss, instead:\n",
    "    # 1) L_recon is the MSE between X_t and the prediction of X_t\n",
    "    # 2) L_pred is the MSE between X_t+m and the prediction of X_t+m, over m timesteps\n",
    "    # 3) L_lin is the MSE between encoder(X_t+m) and the prediction of encoder(X_t+m), over m timesteps\n",
    "    def Loss(yTrue, yPred):\n",
    "        L_recon = Lambda(S_error)([x_in[:,:,:xmax_state-num_control], decoded_x[:,:,:xmax_state-num_control]])\n",
    "        # According to ULE paper, this should be over 30 steps, instead of 50.\n",
    "        L_pred = tf.reduce_mean(tf.math.squared_difference(\n",
    "            # these zeros are same as NUM_HISTORICAL_LOSS_PREDICTION_STEPS\n",
    "            yTrue[:,0,:xmax_state-num_control],  \n",
    "            yPred[:,0,:xmax_state-num_control]))\n",
    "        L_lin = tf.reduce_mean(tf.math.squared_difference(encoder(yTrue),KGx))   \n",
    "        return  ((par['alpha_1']  * (L_recon+L_pred)) + L_lin)\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from Architecture_old import _transformer,_pred_K, linear_update\n",
    "\n",
    "'''\n",
    "Input\n",
    "'''\n",
    "x_in = Input(input_shape)\n",
    "'''\n",
    "Encoder part\n",
    "'''\n",
    "Gx = _transformer(x_in, par['latent dim'], par['en dim list'],par['alpha_3'],activation_out='linear')\n",
    "encoder = Model(x_in, Gx)\n",
    "\n",
    "'''\n",
    "linear update in latent space: Predicting via Koopman eigenvalues including control\n",
    "'''\n",
    "Koop = _pred_K(Gx, par['num complex'], par['num real'],par['hidden_widths_omega'], par['K reg'],par['alpha_3'],activation_out='linear')\n",
    "LU = linear_update(output_dim = latent_shape, num_complex = par['num complex'], num_real = par['num real'])\n",
    "KGx = LU([Gx, Koop])\n",
    "Knet = Model(x_in, [Koop, KGx]) \n",
    "\n",
    "'''\n",
    "Decoder part\n",
    "'''\n",
    "decoder_input = Input(shape = Gx.shape[1:])\n",
    "decoded = _transformer(decoder_input, par['output feature dim'], par['de dim list'],par['alpha_3'], activation_out='linear')\n",
    "_decoder = Model(decoder_input, decoded)  \n",
    "'''\n",
    "Outputs\n",
    "'''\n",
    "decoded_x = _decoder(Gx)\n",
    "decoded_xp = _decoder(KGx)\n",
    "'''\n",
    "Full model\n",
    "'''\n",
    "full_model = Model(x_in, decoded_xp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a  custom Knet to predict on the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom Koopman network with input: y_t, and output: y_t+1\n",
    "In contrast to standard Knet defined above, which takes: input x_t and output: y_t+1 \n",
    "This allows for multi-step predictions in the latent space\n",
    "'''\n",
    "# Disassemble layers\n",
    "layers = [l for l in Knet.layers]\n",
    "\n",
    "# Create a new layer thats the latent input (y_t), instead of the state (x_t)\n",
    "latent_input = Input(shape = Gx.shape[1:])\n",
    "\n",
    "# starting layer\n",
    "s_layer = len(encoder.layers)\n",
    "\n",
    "# # When _pred_K uses only first timesteps of Gx to compute Koop\n",
    "# Next layer directly connects this latent layer to the Pred_K function, bypassing the encoder\n",
    "# This is the same as the Pred_K function in Architecture.py\n",
    "x = layers[s_layer](latent_input)# auxilary dense layer\n",
    "Koop_l = layers[s_layer+1](x)#Dense\n",
    "\n",
    "# Define new linear update\n",
    "KGx_l = LU([latent_input, Koop_l])\n",
    "custom_Knet = Model(latent_input, [Koop_l, KGx_l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1, 4)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1, 80)             400       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1, 80)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 80)             6480      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 80)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 20)             1620      \n",
      "=================================================================\n",
      "Total params: 8,500\n",
      "Trainable params: 8,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 4)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 80)        400         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1, 80)        0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 80)        6480        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1, 80)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 20)        1620        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 1, 170)       3570        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 1, 20)        3420        time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "linear_update (linear_update)   (None, 1, 20)        0           dense_2[0][0]                    \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 15,490\n",
      "Trainable params: 15,490\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 1, 20)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 1, 170)       3570        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 1, 20)        3420        time_distributed[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "linear_update (linear_update)   (None, 1, 20)        0           input_3[0][0]                    \n",
      "                                                                 time_distributed_1[1][0]         \n",
      "==================================================================================================\n",
      "Total params: 6,990\n",
      "Trainable params: 6,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1, 20)]           0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1, 80)             1680      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1, 80)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1, 80)             6480      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1, 80)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1, 4)              324       \n",
      "=================================================================\n",
      "Total params: 8,484\n",
      "Trainable params: 8,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_3\"\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             [(None, 1, 4)]        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 1, 80)         400         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation (Activation)          (None, 1, 80)         0           dense[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1, 80)         6480        activation[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1, 80)         0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1, 20)         1620        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribute (None, 1, 170)        3570        dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 1, 20)         3420        time_distributed[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "linear_update (linear_update)    (None, 1, 20)         0           dense_2[0][0]                    \n",
      "                                                                   time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "model_2 (Functional)             (None, 1, 4)          8484        linear_update[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 23,974\n",
      "Trainable params: 23,974\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Models\n",
    "'''\n",
    "print(encoder.summary())\n",
    "print(Knet.summary())\n",
    "print(custom_Knet.summary())\n",
    "print(_decoder.summary())\n",
    "print(full_model.summary(line_length=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "training\n",
    "'''   \n",
    "# make a folder for the model checkpoints\n",
    "os.mkdir('models') if not os.path.isdir('models') else None\n",
    "epoch_ck_dir = 'models/'+datetime.now().strftime(\"%Y.%m.%d.%H.%M.%S\")\n",
    "os.mkdir(epoch_ck_dir)\n",
    "\n",
    "# Define tensorboard functions\n",
    "log_dir = \"tensorboard_logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,write_images=True,histogram_freq=0)\n",
    "\n",
    "#callbacks\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(epoch_ck_dir+'/weights_epoch_{epoch:08d}',\n",
    "                                                               save_weights_only=False,save_freq='epoch')\n",
    "\n",
    "callbacks = [\n",
    "                    model_checkpoint_callback,\n",
    "                    tensorboard_callback,\n",
    "                    tf.keras.callbacks.CSVLogger(epoch_ck_dir+'/history.csv'),\n",
    "                ]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir tensorboard_logs/fit  --port=6007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 23:45:58.909459: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-24 23:45:58.909860: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-24 23:45:58.909966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:45:58.910360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 80 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 849.46GiB/s\n",
      "2022-03-24 23:45:58.910381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-24 23:45:58.910394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-24 23:45:58.910400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-24 23:45:58.910405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-24 23:45:58.910411: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-24 23:45:58.910416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-24 23:45:58.910421: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-24 23:45:58.910426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-24 23:45:58.910466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:45:58.910852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:45:58.911185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-03-24 23:45:58.911203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-24 23:45:59.164461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-03-24 23:45:59.164481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-03-24 23:45:59.164485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-03-24 23:45:59.164626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:45:59.165023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:45:59.165405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:45:59.165754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\n",
      "2022-03-24 23:45:59.174901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "2022-03-24 23:45:59.198171: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3504000000 Hz\n"
     ]
    }
   ],
   "source": [
    "optimizer=Adam(lr = par['lr'],decay = par['lr']/par['epochs'])\n",
    "\n",
    "# Compile model\n",
    "full_model.compile(loss=customLoss(),\n",
    "                   metrics=[\n",
    "                       State_loss, Rec_loss,Rec_plus1_loss,\n",
    "                   ],\n",
    "                   optimizer=optimizer,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Either carry on from pretraining, or start new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 23:46:00.047248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35/787 [>.............................] - ETA: 3s - batch: 17.0000 - size: 128.0000 - loss: 0.0021 - State_loss: 0.0017 - Rec_loss: 0.2068 - Rec_plus1_loss: 0.2071   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 23:46:00.348913: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-24 23:46:00.353971: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-03-24 23:46:00.358264: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-03-24 23:46:00.358276: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-03-24 23:46:00.358286: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs\n",
      "2022-03-24 23:46:00.358391: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcupti.so.11.0'; dlerror: libcupti.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/rl-lab/dkncem_environment/lib/python3.8/site-packages/cv2/../../lib64::/usr/local/cuda-11.0/targets/x86_64-linux/lib:/usr/local/cuda-11.0/targets/x86_64-linux/lib\n",
      "2022-03-24 23:46:00.358433: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcupti.so'; dlerror: libcupti.so: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/rl-lab/dkncem_environment/lib/python3.8/site-packages/cv2/../../lib64::/usr/local/cuda-11.0/targets/x86_64-linux/lib:/usr/local/cuda-11.0/targets/x86_64-linux/lib\n",
      "2022-03-24 23:46:00.358439: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-03-24 23:46:00.366464: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2022-03-24 23:46:00.366504: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1496] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-03-24 23:46:00.368820: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2022-03-24 23:46:00.369535: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2022-03-24 23:46:00.370152: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00\n",
      "2022-03-24 23:46:00.370306: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00/rllab-Z590-S01.trace.json.gz\n",
      "2022-03-24 23:46:00.379823: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00\n",
      "2022-03-24 23:46:00.382584: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00/rllab-Z590-S01.memory_profile.json.gz\n",
      "2022-03-24 23:46:00.382674: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00Dumped tool data for xplane.pb to tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00/rllab-Z590-S01.xplane.pb\n",
      "Dumped tool data for overview_page.pb to tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00/rllab-Z590-S01.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00/rllab-Z590-S01.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00/rllab-Z590-S01.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to tensorboard_logs/fit/20220324-234558/plugins/profile/2022_03_24_23_46_00/rllab-Z590-S01.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778/787 [============================>.] - ETA: 0s - batch: 388.5000 - size: 127.9152 - loss: 1.3670e-04 - State_loss: 9.4791e-05 - Rec_loss: 0.0206 - Rec_plus1_loss: 0.0213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rl-lab/dkncem_environment/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2022-03-24 23:46:04.206168: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-24 23:46:04.206287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:46:04.206426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 80 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 849.46GiB/s\n",
      "2022-03-24 23:46:04.206444: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-24 23:46:04.206456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-24 23:46:04.206460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-24 23:46:04.206465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-24 23:46:04.206469: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-24 23:46:04.206474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-24 23:46:04.206478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-24 23:46:04.206483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-24 23:46:04.206518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:46:04.206635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:46:04.206727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-03-24 23:46:04.206742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-03-24 23:46:04.206745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-03-24 23:46:04.206747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-03-24 23:46:04.206787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:46:04.206901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-24 23:46:04.207024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9692 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\n",
      "2022-03-24 23:46:04.790596: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/2022.03.24.23.45.58/weights_epoch_00000001/assets\n",
      "787/787 [==============================] - 7s 7ms/step - batch: 393.0000 - size: 127.9161 - loss: 1.3524e-04 - State_loss: 9.3765e-05 - Rec_loss: 0.0204 - Rec_plus1_loss: 0.0210 - val_loss: 8.8132e-06 - val_State_loss: 5.0630e-06 - val_Rec_loss: 0.0018 - val_Rec_plus1_loss: 0.0020\n",
      "Epoch 2/25\n",
      "775/787 [============================>.] - ETA: 0s - batch: 387.0000 - size: 127.9148 - loss: 5.8153e-06 - State_loss: 3.5215e-06 - Rec_loss: 0.0010 - Rec_plus1_loss: 0.0013INFO:tensorflow:Assets written to: models/2022.03.24.23.45.58/weights_epoch_00000002/assets\n",
      "787/787 [==============================] - 6s 8ms/step - batch: 393.0000 - size: 127.9161 - loss: 5.7771e-06 - State_loss: 3.5041e-06 - Rec_loss: 0.0010 - Rec_plus1_loss: 0.0013 - val_loss: 3.3591e-06 - val_State_loss: 2.4196e-06 - val_Rec_loss: 3.5169e-04 - val_Rec_plus1_loss: 5.8775e-04\n",
      "Epoch 3/25\n",
      "779/787 [============================>.] - ETA: 0s - batch: 389.0000 - size: 127.9153 - loss: 2.6693e-06 - State_loss: 1.8892e-06 - Rec_loss: 2.6731e-04 - Rec_plus1_loss: 5.1273e-04"
     ]
    }
   ],
   "source": [
    "# Carry on training\n",
    "\n",
    "history = full_model.fit(train_generator,epochs = EPOCHS, verbose=1,\n",
    "                validation_data=valid_generator,\n",
    "                validation_steps=VALIDATION_STEPS,\n",
    "                steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                callbacks = callbacks,\n",
    "                #workers=16,\n",
    "                #use_multiprocessing=True,\n",
    "               )\n",
    "history = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Check losses\n",
    "'''\n",
    "\n",
    "# training loss\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.figure()\n",
    "plt.plot((history['loss']), 'r')\n",
    "plt.plot((history['State_loss']),'k')\n",
    "plt.plot((history['Rec_loss']), 'b')\n",
    "plt.plot((history['Rec_plus1_loss']), 'g')\n",
    "plt.plot((history['val_loss']), '--r')\n",
    "plt.plot((history['val_State_loss']),'--k')\n",
    "plt.plot((history['val_Rec_loss']), '--b')\n",
    "plt.plot((history['val_Rec_plus1_loss']), '--g')\n",
    "\n",
    "plt.legend(['loss', 'State_loss', 'Rec_loss','Rec_plus1_loss','val_loss', 'val_State_loss', 'val_Rec_loss','val_Rec_plus1_loss',\n",
    "           ])\n",
    "plt.yscale('log')\n",
    "print(history['State_loss'][::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the input/output data and the next step prediction\n",
    "def plot_predictions(dataset_name, num_batches):\n",
    "    dataset = {\n",
    "      'train_generator': lambda x: train_generator,\n",
    "      'valid_generator': lambda x: valid_generator,\n",
    "      'test_generator': lambda x: test_generator\n",
    "    }[dataset_name](x)\n",
    "    \n",
    "    \n",
    "    for j in range(num_batches):# plot a number of batches\n",
    "    \n",
    "        # Next step prediction for a batch\n",
    "        xhat_tp1 = full_model.predict(dataset[j][0])\n",
    "    \n",
    "        for i in range(BATCH_SIZE):\n",
    "            plt.plot(np.squeeze([dataset[j][0][i,:,0],dataset[j][1][i,:,0]]),\n",
    "                     np.squeeze([dataset[j][0][i,:,1],dataset[j][1][i,:,1]]),\n",
    "                     np.squeeze([dataset[j][0][i,:,2],dataset[j][1][i,:,2]]),\n",
    "                    '-b', label = 'x_t->x_t+1' if i == 0 and j==0 else \"\")\n",
    "            plt.plot(np.squeeze([dataset[j][0][i,:,0],xhat_tp1[i,:,0]]),\n",
    "                    np.squeeze([dataset[j][0][i,:,1],xhat_tp1[i,:,1]]),\n",
    "                    np.squeeze([dataset[j][0][i,:,2],xhat_tp1[i,:,2]]),\n",
    "                    '-r', label = 'x_t->xhat_t+1' if i == 0 and j==0 else \"\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Theta')\n",
    "    plt.ylabel('dTheta')\n",
    "    plt.title(dataset_name)\n",
    "    \n",
    "num_batches = 2\n",
    "fig = plt.figure(figsize=plt.figaspect(0.333))\n",
    "ax = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "plot_predictions('train_generator',num_batches)\n",
    "ax = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "plot_predictions('valid_generator',num_batches)\n",
    "ax = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "plot_predictions('test_generator',num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dkncem_environment')",
   "language": "python",
   "name": "python3810jvsc74a57bd04048412d913552f3783794819c40fd96c30d8f34c8bd104c4751b5d84dd1043c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
